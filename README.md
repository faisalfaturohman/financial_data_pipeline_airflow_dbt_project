# Financial Data Pipeline Solution â€“ Analytics Engineer Technical Test

## ğŸ“Œ Overview
This project implements a **complete financial data pipeline** leveraging modern data engineering tools for orchestration, transformation, and analytics.

**Key Technologies Used:**
- **Apache Airflow** â€“ Workflow orchestration
- **dbt** â€“ Data transformation and modeling
- **PostgreSQL** â€“ Data storage
- **Docker** â€“ Containerization for reproducible environments

The solution covers:
- **Data ingestion** from CSV sources
- **Layered dbt transformations** (staging â†’ intermediate â†’ marts)
- **Data quality checks**
- **Performance optimizations**
- **Automated testing & documentation**

---

## ğŸ“‚ Project Structure
```
financial_data_pipeline/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ financial_data_pipeline_dag.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ financial_pipeline/
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ profiles.yml
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_transactions.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_accounts.sql
â”‚       â”‚   â”‚   â””â”€â”€ stg_customers.sql
â”‚       â”‚   â”œâ”€â”€ intermediate/
â”‚       â”‚   â”‚   â”œâ”€â”€ int_transactions_with_accounts.sql
â”‚       â”‚   â”‚   â””â”€â”€ int_daily_transaction_volumes.sql
â”‚       â”‚   â””â”€â”€ marts/
â”‚       â”‚       â””â”€â”€ finance_transaction_fact.sql
â”‚       â”œâ”€â”€ macros/
â”‚       â””â”€â”€ tests/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ transactions.csv
â”‚   â”œâ”€â”€ accounts.csv
â”‚   â””â”€â”€ customers.csv
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Data Sources
Sample CSV datasets provided in `/data/`:
- **transactions.csv** â€“ Transaction details  
- **accounts.csv** â€“ Account details  
- **customers.csv** â€“ Customer demographic info  

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Prerequisites
- Docker & Docker Compose installed
- Python 3.9+ (if running locally outside Docker)

### 2ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/yourusername/financial_data_pipeline.git
cd financial_data_pipeline
```

### 3ï¸âƒ£ Build & Start Services
```bash
docker-compose up --build
```
This will start:
- **Airflow Webserver** at `http://localhost:8080`
- **PostgreSQL** database
- **Airflow Scheduler**

---

## ğŸš€ Pipeline Workflow

### **Airflow DAG:** `financial_data_pipeline`
Daily scheduled pipeline with the following steps:

1. **Create Raw Tables** â€“ `PostgresOperator` creates schemas & tables in `raw` schema.  
2. **Load CSV Data** â€“ Python tasks ingest data into PostgreSQL.  
3. **Run dbt Models** â€“ Transformations executed in layered structure:
   - **Staging:** Data cleaning & standardization  
   - **Intermediate:** Business logic & enrichment  
   - **Marts:** Final fact table for analytics  
4. **Run dbt Tests** â€“ Data quality checks (unique, not null, referential integrity).  
5. **Generate Documentation** â€“ dbt docs for lineage & model descriptions.

---

## ğŸ—ï¸ dbt Architecture

### **Layers**
- **Staging:** `stg_*` models clean raw data and add quality flags.
- **Intermediate:** Join staging tables, enrich data (currency conversion, categorization).
- **Marts:** Create `finance_transaction_fact` table for reporting.

### **Materialization Strategy**
- **Staging & Intermediate:** Views for flexibility
- **Marts:** Tables for performance

---

## âœ… Data Quality & Testing
Implemented with dbt tests:
- **Uniqueness** and **not null** checks
- **Foreign key relationships**
- **Business rules** (e.g., no future transaction dates, zero amount flags)

---

## ğŸ“ˆ Performance Considerations
- Indexed `transaction_date`, `customer_id`, `account_id`, `transaction_type`, `amount_usd` in marts
- Early filtering in staging models
- Efficient joins with proper keys
- Fixed materialization strategies for optimized performance

---

## ğŸ“Œ Key Insights
- **Data Quality Issues:** Handled with flags for anomalies
- **Currency Conversion:** Fixed-rate USD conversion
- **Customer Segmentation:** Based on age and account maturity
- **Transaction Analysis:** Daily aggregation for patterns

---

## ğŸ§ª Running dbt Commands (Inside Airflow Container)
```bash
# Run models
cd /opt/airflow/dbt/financial_pipeline
dbt run --profiles-dir /opt/airflow/dbt

# Test models
dbt test --profiles-dir /opt/airflow/dbt

# Generate docs
dbt docs generate --profiles-dir /opt/airflow/dbt
dbt docs serve --profiles-dir /opt/airflow/dbt
```

---

## ğŸ–¥ï¸ Accessing Services
- **Airflow Web UI:** [http://localhost:8080](http://localhost:8080) (default login: `airflow/airflow`)
- **PostgreSQL:** `localhost:5432` (user: `airflow`, pass: `airflow`)

---

## ğŸ“œ License
This project is released under the MIT License.
